{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007BFF; height: 4px; width: 100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Longitudinal Prediction**\n",
    "\n",
    "Can we use previous year's data and compoundingly predict suicide risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007BFF; height: 4px; width: 100%;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yearly_columns(base_cols, years):\n",
    "    return [f\"{col}{year}\" for col in base_cols for year in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_columns(df, variables):\n",
    "    cols = set()\n",
    "    for var in variables:\n",
    "        cs = df.filter(regex=fr\"^{var}.+\").columns.tolist()\n",
    "        for c in cs:\n",
    "            match = re.search(r\"(.+)(\\d{4})\", c)\n",
    "            if match:\n",
    "                c_name, _ = match.groups()\n",
    "                cols.add(c_name)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = range(2023, 2027)\n",
    "years = range(2020, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read formatted linked persons\n",
    "final23 = pd.read_csv(\"processed/final23.csv\")\n",
    "final24 = pd.read_csv(\"processed/final24.csv\")\n",
    "final25 = pd.read_csv(\"processed/final25.csv\")\n",
    "final26 = pd.read_csv(\"processed/final26.csv\")\n",
    "all_linked = pd.concat([final23, final24, final25, final26], ignore_index=True, axis=0)\n",
    "\n",
    "final = {\n",
    "    2023: final23,\n",
    "    2024: final24,\n",
    "    2025: final25,\n",
    "    2026: final26,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read encoded linked persons\n",
    "encoded23 = pd.read_csv(\"encoded/encoded23.csv\")\n",
    "encoded24 = pd.read_csv(\"encoded/encoded24.csv\")\n",
    "encoded25 = pd.read_csv(\"encoded/encoded25.csv\")\n",
    "encoded26 = pd.read_csv(\"encoded/encoded26.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    final23[f\"HasResponse{year}\"] = final23[f\"HasResponse{year}\"].replace(-1, 0)\n",
    "    encoded23[f\"HasResponse{year}\"] = encoded23[f\"HasResponse{year}\"].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions sorted by categorical, ordinal, and quantitative\n",
    "\n",
    "ordinal_cols = {\n",
    "    \"INV.DBL.APSS.Q1.MindReading\",\n",
    "    \"INV.DBL.APSS.Q2.TVRadio\",\n",
    "    \"INV.DBL.APSS.Q3.Spying\",\n",
    "    \"INV.DBL.APSS.Q4.Auditory\",\n",
    "    \"INV.DBL.APSS.Q5.Controlled\",\n",
    "    \"INV.DBL.APSS.Q6.Visual\",\n",
    "    \"INV.DBL.APSS.Q7.Grandiosity\",\n",
    "    \"INV.INT.ERS.Q01.Persistence1\",\n",
    "    \"INV.INT.ERS.Q02.Sensitivity1\",\n",
    "    \"INV.INT.ERS.Q03.IntensityArousal1\",\n",
    "    \"INV.INT.ERS.Q04.IntensityArousal2\",\n",
    "    \"INV.INT.ERS.Q05.Sensitivity2\",\n",
    "    \"INV.INT.ERS.Q06.IntensityArousal3\",\n",
    "    \"INV.INT.ERS.Q07.Sensitivity3\",\n",
    "    \"INV.INT.ERS.Q08.Persistence2\",\n",
    "    \"INV.INT.ERS.Q09.Sensitivity4\",\n",
    "    \"INV.INT.ERS.Q10.Persistence3\",\n",
    "    \"INV.INT.ERS.Q11.Persistence4\",\n",
    "    \"INV.INT.ERS.Q12.Sensitivity5\",\n",
    "    \"INV.INT.ERS.Q13.Sensitivity6\",\n",
    "    \"INV.INT.ERS.Q14.Sensitivity7\",\n",
    "    \"INV.INT.ERS.Q15.Sensitivity8\",\n",
    "    \"INV.INT.ERS.Q16.Sensitivity9\",\n",
    "    \"INV.INT.ERS.Q17.IntensityArousal4\",\n",
    "    \"INV.INT.ERS.Q18.Sensitivity10\",\n",
    "    \"INV.INT.ERS.Q19.IntensityArousal5\",\n",
    "    \"INV.INT.ERS.Q20.IntensityArousal6\",\n",
    "    \"INV.INT.ERS.Q21.IntensityArousal7\",\n",
    "    \"INV.INT.PHQ4.Q1.Anxious\",\n",
    "    \"INV.INT.PHQ4.Q2.Worried\",\n",
    "    \"INV.INT.PHQ4.Q3.Depressed\",\n",
    "    \"INV.INT.PHQ4.Q4.Anhedonia\",\n",
    "    \"INV.INT.SUB.Alcohol.Past30\",\n",
    "    \"INV.INT.SUB.Cannabis.Past30\",\n",
    "    \"INV.INT.SUB.Cigarettes.Past30\",\n",
    "    \"INV.INT.SUB.Cigars.Past30\",\n",
    "    \"INV.INT.SUB.Smokeless.Past30\",\n",
    "    \"INV.INT.SUB.Vapes.Past30\",\n",
    "}\n",
    "\n",
    "quant_cols = {\n",
    "    \"INV.DBL.APSS.Total\",\n",
    "    \"INV.INT.ERS.IntensityArousalTotal\",\n",
    "    \"INV.INT.ERS.PersistenceTotal\",\n",
    "    \"INV.INT.ERS.SensitivityTotal\",\n",
    "    \"INV.INT.ERS.Total\",\n",
    "    \"INV.INT.PHQ4.Anxiety\",\n",
    "    \"INV.INT.PHQ4.Depression\",\n",
    "    \"INV.INT.PHQ4.Total\",\n",
    "}\n",
    "\n",
    "cat_cols = {\n",
    "    \"INV.LGL.PHQ4.Anxiety\",\n",
    "    \"INV.LGL.PHQ4.Depression\",\n",
    "    \"INV.INT.SI.Attempt\",\n",
    "    \"INV.INT.SI.How\",\n",
    "    \"INV.INT.SI.Selfharm\",\n",
    "    \"INV.INT.SI.Thoughts\",\n",
    "    \"INV.LGL.SUB.Alcohol.Life\",\n",
    "    \"INV.LGL.SUB.Cannabis.Life\",\n",
    "    \"INV.LGL.SUB.Cigarettes.Life\",\n",
    "    \"INV.LGL.SUB.Cigars.Life\",\n",
    "    \"INV.LGL.SUB.Smokeless.Life\",\n",
    "    \"INV.LGL.SUB.Vapes.Life\",\n",
    "    \"INV.LGL.HelpSeeking0\",\n",
    "    \"INV.LGL.HelpSeeking1\",\n",
    "    \"INV.LGL.HelpSeeking2\",\n",
    "    \"INV.LGL.HelpSeeking3\",\n",
    "    \"INV.LGL.HelpSeeking4\",\n",
    "    \"INV.LGL.HelpSeeking5\",\n",
    "    \"INV.LGL.HelpSeeking6\",\n",
    "    \"INV.LGL.HelpSeeking7\",\n",
    "    \"INV.LGL.HelpSeeking8\",\n",
    "    \"INV.LGL.HelpSeeking9\",\n",
    "    \"INV.LGL.HelpSeeking10\",\n",
    "    \"INV.LGL.HelpSeeking11\",\n",
    "    \"INV.LGL.HelpSeeking12\",\n",
    "    \"INV.LGL.HelpSeeking13\",\n",
    "    \"SBJ.FCT.Ethnicity\",\n",
    "    \"SBJ.FCT.GenderId\",\n",
    "    \"SBJ.FCT.Sex\",\n",
    "    \"SBJ.FCT.Sexuality\",\n",
    "    \"SBJ.LGL.Race.AmericanIndianAlaskaNative\",\n",
    "    \"SBJ.LGL.Race.Asian\",\n",
    "    \"SBJ.LGL.Race.HaitianBlackAfricanAmerican\",\n",
    "    \"SBJ.LGL.Race.HawaiianPacificIslander\",\n",
    "    \"SBJ.LGL.Race.MiddleEasternNorthAfrican\",\n",
    "    \"SBJ.LGL.Race.White\",\n",
    "    \"SBJ.LGL.Race.Multiple\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All common questions across years\n",
    "\n",
    "# Survey\n",
    "survey = [\n",
    "    \"Unnamed: 0\",\n",
    "    \"IDX.INT.Origin.Database\",\n",
    "    \"IDX.INT.Origin.Record\",\n",
    "    \"SSS.INT.SurveyYear\",\n",
    "    \"IDX.INT.Row\",\n",
    "    \"IDX.CHR.Linked.ID\",\n",
    "    \"QCC.LGC.Linked.Attempted\",\n",
    "    \"QCC.LGC.Linked\",\n",
    "    \"QCC.LGC.Linked.No_issues\",\n",
    "    \"QCC.CHR.Linked.Score.Base\",\n",
    "    \"QCC.CHR.Linked.Score.Add\",\n",
    "    \"QCC.CHR.Linked.Rows\",\n",
    "    \"QCC.CHR.Linked.Dissimilarity\",\n",
    "]\n",
    "   \n",
    "# Adolescent Psychotic-like Symptoms \n",
    "psychotic = [\n",
    "    \"INV.DBL.APSS.Q1.MindReading\",\n",
    "    \"INV.DBL.APSS.Q2.TVRadio\",\n",
    "    \"INV.DBL.APSS.Q3.Spying\",\n",
    "    \"INV.DBL.APSS.Q4.Auditory\",\n",
    "    \"INV.DBL.APSS.Q5.Controlled\",\n",
    "    \"INV.DBL.APSS.Q6.Visual\",\n",
    "    \"INV.DBL.APSS.Q7.Grandiosity\",\n",
    "    \"INV.DBL.APSS.Total\",\n",
    "]\n",
    "    \n",
    "# Emotional Reactivity \n",
    "emotional = [\n",
    "    \"INV.INT.ERS.IntensityArousalTotal\",\n",
    "    \"INV.INT.ERS.PersistenceTotal\",\n",
    "    \"INV.INT.ERS.Q01.Persistence1\",\n",
    "    \"INV.INT.ERS.Q02.Sensitivity1\",\n",
    "    \"INV.INT.ERS.Q03.IntensityArousal1\",\n",
    "    \"INV.INT.ERS.Q04.IntensityArousal2\",\n",
    "    \"INV.INT.ERS.Q05.Sensitivity2\",\n",
    "    \"INV.INT.ERS.Q06.IntensityArousal3\",\n",
    "    \"INV.INT.ERS.Q07.Sensitivity3\",\n",
    "    \"INV.INT.ERS.Q08.Persistence2\",\n",
    "    \"INV.INT.ERS.Q09.Sensitivity4\",\n",
    "    \"INV.INT.ERS.Q10.Persistence3\",\n",
    "    \"INV.INT.ERS.Q11.Persistence4\",\n",
    "    \"INV.INT.ERS.Q12.Sensitivity5\",\n",
    "    \"INV.INT.ERS.Q13.Sensitivity6\",\n",
    "    \"INV.INT.ERS.Q14.Sensitivity7\",\n",
    "    \"INV.INT.ERS.Q15.Sensitivity8\",\n",
    "    \"INV.INT.ERS.Q16.Sensitivity9\",\n",
    "    \"INV.INT.ERS.Q17.IntensityArousal4\",\n",
    "    \"INV.INT.ERS.Q18.Sensitivity10\",\n",
    "    \"INV.INT.ERS.Q19.IntensityArousal5\",\n",
    "    \"INV.INT.ERS.Q20.IntensityArousal6\",\n",
    "    \"INV.INT.ERS.Q21.IntensityArousal7\",\n",
    "    \"INV.INT.ERS.SensitivityTotal\",\n",
    "    \"INV.INT.ERS.Total\",\n",
    "]\n",
    "    \n",
    "# PHQ4\n",
    "phq4 = [\n",
    "    \"INV.INT.PHQ4.Anxiety\",\n",
    "    \"INV.INT.PHQ4.Depression\",\n",
    "    \"INV.INT.PHQ4.Q1.Anxious\",\n",
    "    \"INV.INT.PHQ4.Q2.Worried\",\n",
    "    \"INV.INT.PHQ4.Q3.Depressed\",\n",
    "    \"INV.INT.PHQ4.Q4.Anhedonia\",\n",
    "    \"INV.INT.PHQ4.Total\",\n",
    "    \"INV.LGL.PHQ4.Anxiety\",\n",
    "    \"INV.LGL.PHQ4.Depression\",\n",
    "    # \"INV.FCT.PHQ4.Total\",\n",
    "]\n",
    "    \n",
    "# Suicidality\n",
    "suicide = [\n",
    "    \"INV.INT.SI.Attempt\",\n",
    "    \"INV.INT.SI.How\",\n",
    "    \"INV.INT.SI.Selfharm\",\n",
    "    \"INV.INT.SI.Thoughts\",\n",
    "]\n",
    "    \n",
    "# Substance Use\n",
    "substance = [\n",
    "    \"INV.INT.SUB.Alcohol.Past30\",\n",
    "    \"INV.INT.SUB.Cannabis.Past30\",\n",
    "    \"INV.INT.SUB.Cigarettes.Past30\",\n",
    "    \"INV.INT.SUB.Cigars.Past30\",\n",
    "    \"INV.INT.SUB.Smokeless.Past30\",\n",
    "    \"INV.INT.SUB.Vapes.Past30\",\n",
    "    \n",
    "    \"INV.LGL.SUB.Alcohol.Life\",\n",
    "    \"INV.LGL.SUB.Cannabis.Life\",\n",
    "    \"INV.LGL.SUB.Cigarettes.Life\",\n",
    "    \"INV.LGL.SUB.Cigars.Life\",\n",
    "    \"INV.LGL.SUB.Smokeless.Life\",\n",
    "    \"INV.LGL.SUB.Vapes.Life\",\n",
    "]\n",
    "    \n",
    "# Help Seeking\n",
    "help = [\n",
    "    \"INV.LGL.HelpSeeking0\",\n",
    "    \"INV.LGL.HelpSeeking1\",\n",
    "    \"INV.LGL.HelpSeeking2\",\n",
    "    \"INV.LGL.HelpSeeking3\",\n",
    "    \"INV.LGL.HelpSeeking4\",\n",
    "    \"INV.LGL.HelpSeeking5\",\n",
    "    \"INV.LGL.HelpSeeking6\",\n",
    "    \"INV.LGL.HelpSeeking7\",\n",
    "    \"INV.LGL.HelpSeeking8\",\n",
    "    \"INV.LGL.HelpSeeking9\",\n",
    "    \"INV.LGL.HelpSeeking10\",\n",
    "    \"INV.LGL.HelpSeeking11\",\n",
    "    \"INV.LGL.HelpSeeking12\",\n",
    "    \"INV.LGL.HelpSeeking13\",\n",
    "    # \"INV.CHR.HelpSeeking.Other\",\n",
    "]\n",
    "    \n",
    "# Identity\n",
    "identity = [\n",
    "    # \"SBJ.CHR.Gender.Other\",\n",
    "    # \"SBJ.CHR.Link.Streetname\",\n",
    "    # \"SBJ.CHR.SexualOrientation.Other\",\n",
    "    \"SBJ.FCT.Ethnicity\",\n",
    "    \"SBJ.FCT.GenderId\",\n",
    "    # \"SBJ.FCT.Link.BirthMonth\",\n",
    "    # \"SBJ.FCT.Link.EyeColor\",\n",
    "    # \"SBJ.FCT.Link.MiddleInitial\",\n",
    "    # \"SBJ.FCT.Link.OlderSiblings\",\n",
    "    # \"SBJ.FCT.Race\",\n",
    "    \"SBJ.FCT.Sex\",\n",
    "    \"SBJ.FCT.Sexuality\",\n",
    "    # \"SBJ.INT.Link.KindergartenYearEst\",\n",
    "    \"SBJ.LGL.Race.AmericanIndianAlaskaNative\",\n",
    "    \"SBJ.LGL.Race.Asian\",\n",
    "    \"SBJ.LGL.Race.HaitianBlackAfricanAmerican\",\n",
    "    \"SBJ.LGL.Race.HawaiianPacificIslander\",\n",
    "    \"SBJ.LGL.Race.MiddleEasternNorthAfrican\",\n",
    "    \"SBJ.LGL.Race.Multiple\",\n",
    "    # \"SBJ.LGL.Race.Other\",\n",
    "    \"SBJ.LGL.Race.White\",\n",
    "]\n",
    "    \n",
    "# School\n",
    "school = [\n",
    "    \"SSS.CHR.GradesSurveyed\",\n",
    "    \"SSS.DBL.PercentOptOutsbyGrades\",\n",
    "    \"SSS.DBL.PercentOptOutsbySchool\",\n",
    "    \"SSS.INT.Cohort\",\n",
    "    \"SSS.INT.District.Code\",\n",
    "    \"SSS.INT.Eighth.Grade.Enrollment\",\n",
    "    \"SSS.INT.Eleventh.Grade.Enrollment\",\n",
    "    \"SSS.INT.Fifth.Grade.Enrollment\",\n",
    "    \"SSS.INT.Grade\",\n",
    "    \"SSS.INT.Ninth.Grade.Enrollment\",\n",
    "    \"SSS.INT.School.Code\",\n",
    "    \"SSS.INT.Seventh.Grade.Enrollment\",\n",
    "    \"SSS.INT.Sixth.Grade.Enrollment\",\n",
    "    \"SSS.INT.Tenth.Grade.Enrollment\",\n",
    "    \"SSS.INT.Time_point\",\n",
    "    \"SSS.INT.Twelfth.Grade.Enrollment\",\n",
    "]\n",
    "\n",
    "all_qs = psychotic + help + suicide + school + identity + substance + phq4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychotic_e = get_encoded_columns(encoded23, psychotic)\n",
    "emotional_e = get_encoded_columns(encoded23, emotional)\n",
    "phq4_e = get_encoded_columns(encoded23, phq4)\n",
    "suicide_e = get_encoded_columns(encoded23, suicide)\n",
    "substance_e = get_encoded_columns(encoded23, substance)\n",
    "help_e = get_encoded_columns(encoded23, help)\n",
    "identity_e = get_encoded_columns(encoded23, identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {\n",
    "    \"regular\": {\n",
    "        \"psychotic\": psychotic,\n",
    "        \"emotional\": emotional,\n",
    "        \"phq4\": phq4,\n",
    "        \"suicide\": suicide,\n",
    "        \"substance\": substance,\n",
    "        \"help\": help,\n",
    "        \"identity\": identity,\n",
    "    },\n",
    "    \"encoded\": {\n",
    "        \"psychotic\": psychotic_e,\n",
    "        \"emotional\": emotional_e,\n",
    "        \"phq4\": phq4_e,\n",
    "        \"suicide\": suicide_e,\n",
    "        \"substance\": substance_e,\n",
    "        \"help\": help_e,\n",
    "        \"identity\": identity_e,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `encoded` should be a string: \"regular\" or \"encoded\"\n",
    "def get_cols(y_year, outcome, qs, encoded, prev):\n",
    "    target_col= f\"INV.INT.SI.{outcome}_1.0{y_year}\" if encoded == \"encoded\" else f\"INV.INT.SI.{outcome}{y_year}\"\n",
    "    \n",
    "    feature_cols = []\n",
    "    for q in qs:\n",
    "        if q != \"suicide\":\n",
    "            feature_cols += get_yearly_columns(cats[encoded][q], [y_year])\n",
    "        if prev:\n",
    "            feature_cols += get_yearly_columns(cats[encoded][q], [y_year-1])\n",
    "    \n",
    "    return feature_cols, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just previous year and curr year\n",
    "def year_one_prediction_data(df, pred_year):\n",
    "    df = df[df[f\"HasResponse{pred_year}\"] == 1]\n",
    "    df = df[df[f\"HasResponse{pred_year + 1}\"] == 1]\n",
    "    \n",
    "    df = df.filter(regex=fr\".*({pred_year}|{pred_year + 1})$\", axis=1)\n",
    "    \n",
    "    df.rename(columns=lambda col: re.sub(fr\"{pred_year}$\", \"0\", col), inplace=True)\n",
    "    df.rename(columns=lambda col: re.sub(fr\"{pred_year + 1}$\", \"1\", col), inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for prev_imputation models\n",
    "df_2022_2023 = year_one_prediction_data(encoded23, 2022)\n",
    "df_2021_2022 = year_one_prediction_data(encoded23, 2021)\n",
    "df_one_year  = pd.concat([df_2022_2023,df_2021_2022],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curr_year_prediction_data(df, curr_year):\n",
    "    df = df[df[f\"HasResponse{curr_year}\"] == 1]\n",
    "    df = df.filter(regex=fr\".*({curr_year})$\", axis=1)\n",
    "    df.rename(columns=lambda col: re.sub(fr\"{curr_year}$\", \"1\", col), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data for curr_imputation models\n",
    "df_2022 = curr_year_prediction_data(encoded23, 2022)\n",
    "df_2023 = curr_year_prediction_data(encoded23, 2023)\n",
    "df_curr  = pd.concat([df_2022, df_2023], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007BFF; height: 4px; width: 100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rational**\n",
    "\n",
    "What variables are actually important/have significance in our prediction model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conditioned_vars(df, explore_qs, condition_q):\n",
    "    filtered_df = df[df[condition_q] != -1]\n",
    "    \n",
    "    num_plots = len(explore_qs)\n",
    "    num_cols = 4\n",
    "    num_rows = math.ceil(num_plots / num_cols) \n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows)) \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, explore_q in enumerate(explore_qs):\n",
    "        if explore_q == condition_q:\n",
    "            continue\n",
    "        \n",
    "        filtered_df = filtered_df[filtered_df[explore_q] != -1]\n",
    "        \n",
    "        num_categories = len(set(filtered_df[explore_q]))\n",
    "        if num_categories <= 3:\n",
    "            count_df = filtered_df.groupby([condition_q, explore_q]).size().reset_index(name=\"Count\")\n",
    "            count_df[\"Proportion\"] = count_df.groupby(condition_q)[\"Count\"].transform(lambda x: x / x.sum())\n",
    "            \n",
    "            sns.barplot(\n",
    "                x=condition_q, y=\"Proportion\", hue=explore_q,\n",
    "                data=count_df, palette=[\"lightcoral\", \"lightblue\"], ax=axes[i]\n",
    "            )\n",
    "            axes[i].set_title(f\"{explore_q}\", fontsize=14)\n",
    "            axes[i].set_xlabel(condition_q, fontsize=12)\n",
    "            axes[i].set_ylabel(\"Proportion\", fontsize=12)\n",
    "            axes[i].set_ylim((0, 1))\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].legend(title=explore_q)\n",
    "        else:\n",
    "            sns.histplot(\n",
    "                data=filtered_df, x=explore_q, hue=condition_q, bins=num_categories, kde=True, \n",
    "                stat=\"density\", element=\"step\", common_norm=False, ax=axes[i], palette=[\"lightcoral\", \"lightblue\"]\n",
    "            )\n",
    "            axes[i].set_title(f\"{explore_q}\", fontsize=14)\n",
    "            axes[i].set_xlabel(explore_q, fontsize=12)\n",
    "            axes[i].set_ylabel(\"Frequency\", fontsize=12)\n",
    "            axes[i].legend(title=condition_q)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncomment the following code to distributions of variables conditioning on a specific suicidality question!\n",
    "\"\"\"\n",
    "# question = \"Selfharm\"\n",
    "# category = suicide\n",
    "\n",
    "# plot_conditioned_vars(final23, get_yearly_columns(category, [2023]), f\"INV.INT.SI.{question}2023\")\n",
    "# plot_conditioned_vars(final23, get_yearly_columns(category, [2022]), f\"INV.INT.SI.{question}2023\")\n",
    "# plot_conditioned_vars(final23, get_yearly_columns(category, [2022]), f\"INV.INT.SI.{question}2022\")\n",
    "# plot_conditioned_vars(final23, get_yearly_columns(category, [2021]), f\"INV.INT.SI.{question}2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007BFF; height: 4px; width: 100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-Tuning Final Models**\n",
    "\n",
    "We are looking into the top models for each suicidality question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mods = [\"XGBoost\", \"RandomForest\", \"LDA\", \"LightGBM\", \"CatBoost\", \"NeuralNet\", \"SupportVector\", \"Logistic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top models:\n",
    "attempt_mods = [\"RandomForest\"]\n",
    "how_mods = [\"RandomForest\"]\n",
    "thoughts_mods = [\"RandomForest\"]\n",
    "selfharm_mods = [\"RandomForest\"]\n",
    "# attempt_mods = [\"RandomForest\", \"LightGBM\", \"NeuralNet\", \"SupportVector\", \"Logistic\"]\n",
    "# how_mods = [\"RandomForest\", \"XGBoost\", \"LightGBM\", \"NeuralNet\", \"Logistic\"]\n",
    "# thoughts_mods = [\"RandomForest\", \"NeuralNet\", \"LightGBM\", \"Logistic\"]\n",
    "# selfharm_mods = [\"RandomForest\", \"LightGBM\", \"NeuralNet\", \"Logistic\"]\n",
    "\n",
    "outcome_models = {\n",
    "    \"Attempt\": attempt_mods,\n",
    "    \"How\": how_mods,\n",
    "    \"Thoughts\": thoughts_mods,\n",
    "    \"Selfharm\": selfharm_mods,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report, precision_recall_curve, average_precision_score, auc\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# models tested\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic\": (\n",
    "        LogisticRegression(random_state=42), \n",
    "        {\n",
    "            'C': np.logspace(-4,4,20),\n",
    "            \"penalty\": ['l1', 'l2', 'elasticnet', 'none'], \n",
    "            \"solver\": ['liblinear', 'lbfgs', 'newton-cg', ], \n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(eval_metric='auc'),\n",
    "        {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42), \n",
    "        {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]}\n",
    "    ),\n",
    "    \"LDA\": (\n",
    "        LinearDiscriminantAnalysis(), \n",
    "        {'solver': ['svd', 'lsqr', 'eigen']}\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(), \n",
    "        {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "    ),\n",
    "    \"CatBoost\": (\n",
    "        CatBoostClassifier(verbose=0), \n",
    "        {'iterations': [50, 100], 'depth': [4, 6]}\n",
    "    ),\n",
    "    \"NeuralNet\": (\n",
    "        MLPClassifier(max_iter=500), \n",
    "        {'hidden_layer_sizes': [(50,), (100,)], 'max_iter': [300, 500]}\n",
    "    ),\n",
    "    \"SupportVector\": (\n",
    "        SVC(probability=True), \n",
    "        {'C': [0.1, 1, 10]}\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ft = {\n",
    "    'Logistic': (\n",
    "        LogisticRegression(random_state=42), \n",
    "        {\n",
    "            'C': np.logspace(-4, 4, 10), \n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'solver': ['liblinear', 'saga'], \n",
    "        }\n",
    "    ),\n",
    "    # \"Logistic\": (\n",
    "    #     LogisticRegression(random_state=42), \n",
    "    #     {\n",
    "    #         'C': np.logspace(-4,4,20),\n",
    "    #         \"penalty\": ['l1', 'l2', 'elasticnet', 'none'], \n",
    "    #         \"solver\": ['liblinear', 'lbfgs', 'newton-cg', ], \n",
    "    #     }\n",
    "    # ),\n",
    "    'RandomForest': (\n",
    "        RandomForestClassifier(random_state=42), \n",
    "        {\n",
    "            'n_estimators': [50, 100, 200],  # Reasonable tree counts\n",
    "            'max_depth': [10, 20, None],  # Limiting the depth for generalization\n",
    "            'min_samples_split': [2, 10],  # Balance between complexity and efficiency\n",
    "        }\n",
    "    ),\n",
    "    # \"RandomForest\": (\n",
    "    #     RandomForestClassifier(random_state=42), \n",
    "    #     {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]}\n",
    "    # ),\n",
    "    \"NeuralNet\": (\n",
    "        MLPClassifier(max_iter=500), \n",
    "        {\n",
    "            'hidden_layer_sizes': [(50,), (100,)],  # Two simple architectures\n",
    "            'activation': ['relu'],  # ReLU is almost always superior for deep learning\n",
    "            'solver': ['adam'],  # Adam is the best general optimizer (removes 'sgd' for efficiency)\n",
    "            'learning_rate_init': [0.001, 0.01],  # Important for stable learning\n",
    "        }\n",
    "    ),\n",
    "    # \"NeuralNet\": (\n",
    "    #     MLPClassifier(max_iter=500), \n",
    "    #     {'hidden_layer_sizes': [(50,), (100,)], 'max_iter': [300, 500]}\n",
    "    # ),\n",
    "    \"SupportVector\": (\n",
    "        SVC(probability=True), \n",
    "        {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "    ),\n",
    "    # \"SupportVector\": (\n",
    "    #     SVC(probability=True), \n",
    "    #     {'C': [0.1, 1, 10]}\n",
    "    # ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(), \n",
    "        {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'num_leaves': [20, 31]}\n",
    "    ),\n",
    "    # \"LightGBM\": (\n",
    "    #     LGBMClassifier(), \n",
    "    #     {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "    # ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(eval_metric='auc'),\n",
    "        {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 6]}\n",
    "    ),\n",
    "    # \"XGBoost\": (\n",
    "    #     XGBClassifier(eval_metric='auc'),\n",
    "    #     {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, outcome, pred_year, qs, encoded, model_params, prev, interactions=False):\n",
    "    print(f\"Fine-tuning models for {outcome}\")\n",
    "    feature_cols, target_col = get_cols(pred_year, outcome, qs, encoded, prev)\n",
    "    \n",
    "    # Filter out people who did not take the survey\n",
    "    df = df[df[f\"HasResponse{pred_year}\"] == 1]\n",
    "    if prev:\n",
    "        df = df[df[f\"HasResponse{pred_year-1}\"] == 1]\n",
    "\n",
    "    X, y = df[feature_cols], df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Get interaction terms\n",
    "    if interactions:\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        X_train_array = poly.fit_transform(X_train)\n",
    "        X_test_array = poly.transform(X_test)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train_array, columns=poly.get_feature_names_out(feature_cols))\n",
    "        X_test = pd.DataFrame(X_test_array, columns=poly.get_feature_names_out(feature_cols))\n",
    "    \n",
    "    X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n",
    "    y_train, y_test = pd.Series(y_train), pd.Series(y_test)\n",
    "    final_probs = X_test.copy() \n",
    "    final_probs[outcome] = y_test.values\n",
    "    \n",
    "    trained_models = {}\n",
    "\n",
    "    # Tune models\n",
    "    outcome_results = []\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6)) \n",
    "    ax_roc, ax_pr = axes\n",
    "    for mod_name in outcome_models[outcome]:\n",
    "        print(f\"Tuning {mod_name} for {outcome}!\")\n",
    "                \n",
    "        mod, param_grid = model_params[mod_name]\n",
    "        grid_search = GridSearchCV(mod, param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Classification\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, target_names=[\"no\", \"yes\"], output_dict=True)\n",
    "        \n",
    "        # PR Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        pr_score = auc(recall, precision)\n",
    "        ax_pr.plot(recall, precision, marker='.', label=f'{mod_name}: {pr_score:.2f}')\n",
    "        \n",
    "        # Plot ROC Curve\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        final_probs[f\"probs{mod_name}\"] = y_pred_proba\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        ax_roc.plot(fpr, tpr, label=f\"{mod_name} (AUC = {auc_score:.3f})\")\n",
    "        \n",
    "        outcome_results.append({\n",
    "            \"Model\": mod_name,\n",
    "            \"Best Params\": grid_search.best_params_,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"ROC AUC\": auc_score,\n",
    "            \"Precision\": report[\"yes\"][\"precision\"],\n",
    "            \"Recall\": report[\"yes\"][\"recall\"],\n",
    "            \"F1\": report[\"yes\"][\"f1-score\"],\n",
    "            \"PR Score\": pr_score,\n",
    "        })\n",
    "        \n",
    "        trained_models[mod_name] = best_model\n",
    "\n",
    "        print(f\"{mod_name} - Best Params: {grid_search.best_params_}, PR: {pr_score:.3f}\")\n",
    "    \n",
    "    # ROC subplot\n",
    "    ax_roc.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line\n",
    "    ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "    ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "    ax_roc.set_title(f\"ROC Curves for {outcome} Models\")\n",
    "    ax_roc.legend(loc=\"lower right\")\n",
    "    ax_roc.grid(True)\n",
    "\n",
    "    # PR subplot\n",
    "    ax_pr.plot([0, 0], [1, 1], linestyle='--', color='gray')  # Reference line\n",
    "    ax_pr.set_xlabel('Recall')\n",
    "    ax_pr.set_ylabel('Precision')\n",
    "    ax_pr.set_ylim(0, 1)\n",
    "    ax_pr.set_title(f'Precision-Recall Curves for {outcome}')\n",
    "    ax_pr.legend()\n",
    "    ax_pr.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame(outcome_results), final_probs, trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncomment the following code to fine-tune a particular model on the data!\n",
    "\"\"\"\n",
    "\n",
    "df = df_one_year\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=24)\n",
    "outcome = \"Attempt\"\n",
    "pred_year = 1\n",
    "qs = [\n",
    "    \"psychotic\", \n",
    "    \"phq4\", \n",
    "    \"suicide\", \n",
    "    \"substance\", \n",
    "    \"help\", \n",
    "    \"identity\"\n",
    "]\n",
    "encoded = \"encoded\"\n",
    "interactions = False\n",
    "model_params = models_ft\n",
    "prev = True\n",
    "\n",
    "# Training and Validation\n",
    "attempt_results, attempt_final_probs, trained_models = train_models(train_data, outcome, pred_year, qs, encoded, model_params, prev, interactions=interactions)\n",
    "display(attempt_results)\n",
    "\n",
    "# ------------------------------\n",
    "# TESTING THE MODEL\n",
    "# ------------------------------\n",
    "final_stats = []\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6)) \n",
    "ax_roc, ax_pr = axes\n",
    "for mod_name, trained_mod in trained_models.items():\n",
    "    feature_cols, target_col = get_cols(pred_year, outcome, qs, encoded, prev)\n",
    "    X, y = df_2022_2023[feature_cols], df_2022_2023[target_col]\n",
    "    y_pred = trained_mod.predict(X)\n",
    "    \n",
    "    # Classification\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    report = classification_report(y, y_pred, target_names=[\"no\", \"yes\"], output_dict=True)\n",
    "    \n",
    "    # PR Curve\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred)\n",
    "    pr_score = auc(recall, precision)\n",
    "    ax_pr.plot(recall, precision, marker='.', label=f'{mod_name}: {pr_score:.2f}')\n",
    "    \n",
    "    # Probabilities\n",
    "    y_pred_proba = trained_mod.predict_proba(X)[:, 1]\n",
    "    roc_score = roc_auc_score(y, y_pred_proba)\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "    ax_roc.plot(fpr, tpr, label=f\"{mod_name} (ROC = {roc_score:.3f})\")\n",
    "    \n",
    "    final_stats.append({\n",
    "        \"Model\": mod_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC AUC\": roc_score,\n",
    "        \"Precision\": report[\"yes\"][\"precision\"],\n",
    "        \"Recall\": report[\"yes\"][\"recall\"],\n",
    "        \"F1\": report[\"yes\"][\"f1-score\"],\n",
    "        \"PR AUC\": pr_score,\n",
    "    })\n",
    "    \n",
    "display(pd.DataFrame(final_stats))\n",
    "\n",
    "# Finalizing the ROC subplot\n",
    "ax_roc.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line\n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "ax_roc.set_title(f\"ROC Curves for {outcome} Models\")\n",
    "ax_roc.legend(loc=\"lower right\")\n",
    "ax_roc.grid(True)\n",
    "\n",
    "# Finalizing the PR subplot\n",
    "ax_pr.plot([0, 0], [1, 1], linestyle='--', color='gray')  # Reference line\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "ax_pr.set_ylim(0, 1)\n",
    "ax_pr.set_title(f'Precision-Recall Curves for {outcome}')\n",
    "ax_pr.legend()\n",
    "ax_pr.grid(True)\n",
    "\n",
    "# Show both subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for name, params in zip(attempt_results[\"Model\"], attempt_results[\"Best Params\"]):\n",
    "    print(f\"{name}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Best Parameters (Curr + Prev Years)**\n",
    "\n",
    "**Predicting \"Attempt\"**\n",
    "- RandomForest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 50, 'num_leaves': 20}\n",
    "- NeuralNet: {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
    "- SupportVector: {'C': 10, 'kernel': 'linear'}\n",
    "- Logistic: {'C': 166.81005372000558, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "**Predicting \"How\"**\n",
    "- RandomForest: {'max_depth': 20, 'n_estimators': 100}\n",
    "- XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
    "- NeuralNet: {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
    "- Logistic: {'C': 21.54434690031882, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "\n",
    "**Predicting \"Thoughts\"**\n",
    "- RandomForest: {'max_depth': None, 'n_estimators': 50}\n",
    "- NeuralNet: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 50, 'num_leaves': 20}\n",
    "- Logistic: {'C': 1291.5496650148827, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "**Predicting \"Self Harm\"**\n",
    "- RandomForest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
    "- NeuralNet: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
    "- Logistic: {'C': 21.54434690031882, 'penalty': 'l1', 'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Best Parameters (Curr Year)**\n",
    "\n",
    "**Predicting \"Attempt\"**\n",
    "- Random Forest: {'max_depth': 20, 'n_estimators': 50}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 20}\n",
    "- Neural Net: {'hidden_layer_sizes': (50,), 'max_iter': 300}\n",
    "- Support Vector: {'C': 10, 'kernel': 'rbf'}\n",
    "- Logistic: {'C': 3792.690190732246, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "**Predicting \"How\"**\n",
    "- Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "- XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
    "- Neural Net: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
    "- Logistic: {'C': 11.288378916846883, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "\n",
    "**Predicting \"Thoughts\"**\n",
    "- Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
    "- Neural Net: {'hidden_layer_sizes': (50,), 'max_iter': 500}\n",
    "- Logistic: {'C': 0.615848211066026, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "\n",
    "**Predicting \"Self Harm\"**\n",
    "- Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
    "- LightGBM: {'learning_rate': 0.1, 'n_estimators': 100}\n",
    "- Neural Net: {'hidden_layer_sizes': (50,), 'max_iter': 500} (recall = 0.724138)\n",
    "- Logistic: {'C': 78.47599703514607, 'penalty': 'l2', 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_ablation(df, outcomes, pred_year, qs, encoded, model_params, prev, interactions=False):\n",
    "    outcome_results = []\n",
    "    trained_models = {}\n",
    "\n",
    "    for outcome in outcomes:\n",
    "        print(f\"Fine-tuning models for {outcome}\")\n",
    "        feature_cols, target_col = get_cols(pred_year, outcome, qs, encoded, prev)\n",
    "        \n",
    "        # Filter out people who did not take the survey\n",
    "        df_filtered = df[df[f\"HasResponse{pred_year}\"] == 1]\n",
    "        if prev:\n",
    "            df_filtered = df_filtered[df_filtered[f\"HasResponse{pred_year-1}\"] == 1]\n",
    "\n",
    "        X, y = df_filtered[feature_cols], df_filtered[target_col]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Get interaction terms if enabled\n",
    "        if interactions:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            X_train = pd.DataFrame(poly.fit_transform(X_train), columns=poly.get_feature_names_out(feature_cols))\n",
    "            X_test = pd.DataFrame(poly.transform(X_test), columns=poly.get_feature_names_out(feature_cols))\n",
    "\n",
    "        # Convert to Pandas DataFrame for consistency\n",
    "        X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n",
    "        y_train, y_test = pd.Series(y_train), pd.Series(y_test)\n",
    "        final_probs = X_test.copy()\n",
    "        final_probs[outcome] = y_test.values\n",
    "\n",
    "        # Create subplots for ROC & PR Curves\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6)) \n",
    "        ax_roc, ax_pr = axes\n",
    "\n",
    "        for mod_name in model_params:\n",
    "            print(f\"Tuning {mod_name} for {outcome}!\")\n",
    "\n",
    "            mod, param_grid = model_params[mod_name]\n",
    "            grid_search = GridSearchCV(mod, param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            # Classification\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred, target_names=[\"no\", \"yes\"], output_dict=True)\n",
    "\n",
    "            # PR Curve\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "            pr_score = auc(recall, precision)\n",
    "            ax_pr.plot(recall, precision, marker='.', label=f'{mod_name}: {pr_score:.2f}')\n",
    "\n",
    "            # ROC Curve\n",
    "            y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            ax_roc.plot(fpr, tpr, label=f\"{mod_name} (AUC = {auc_score:.3f})\")\n",
    "\n",
    "            # Store results\n",
    "            outcome_results.append({\n",
    "                \"Outcome\": outcome,\n",
    "                \"Model\": mod_name,\n",
    "                \"Best Params\": grid_search.best_params_,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"ROC AUC\": auc_score,\n",
    "                \"Precision\": report[\"yes\"][\"precision\"],\n",
    "                \"Recall\": report[\"yes\"][\"recall\"],\n",
    "                \"F1\": report[\"yes\"][\"f1-score\"],\n",
    "                \"PR Score\": pr_score,\n",
    "            })\n",
    "\n",
    "            trained_models[f\"{mod_name}_{outcome}\"] = best_model\n",
    "            print(f\"{mod_name}_{outcome} - Best Params: {grid_search.best_params_}, PR AUC: {pr_score:.3f}\")\n",
    "\n",
    "        # Finalize ROC subplot\n",
    "        ax_roc.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal reference line\n",
    "        ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "        ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "        ax_roc.set_title(f\"ROC Curves for {outcome} Models\")\n",
    "        ax_roc.legend(loc=\"lower right\")\n",
    "        ax_roc.grid(True)\n",
    "\n",
    "        # Finalize PR subplot\n",
    "        ax_pr.plot([0, 0], [1, 1], linestyle='--', color='gray')  # Reference line\n",
    "        ax_pr.set_xlabel('Recall')\n",
    "        ax_pr.set_ylabel('Precision')\n",
    "        ax_pr.set_ylim(0, 1)\n",
    "        ax_pr.set_title(f'Precision-Recall Curves for {outcome}')\n",
    "        ax_pr.legend()\n",
    "        ax_pr.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return pd.DataFrame(outcome_results), final_probs, trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# TRAINING AND TESTING PHASE\n",
    "# ------------------------------\n",
    "\n",
    "df = df_curr\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=24)\n",
    "outcomes = [\"Attempt\", \"How\", \"Thoughts\", \"Selfharm\"]\n",
    "pred_year = 1\n",
    "qs = [\"psychotic\", \"phq4\", \"suicide\", \"substance\", \"help\", \"identity\"]\n",
    "encoded = \"encoded\"\n",
    "interactions = False\n",
    "model_params = models\n",
    "prev = False\n",
    "\n",
    "# Training and Validation\n",
    "attempt_results, attempt_final_probs, trained_models = train_models_ablation(train_data, outcomes, pred_year, qs, encoded, model_params, prev, interactions=interactions)\n",
    "display(attempt_results)\n",
    "\n",
    "# ------------------------------\n",
    "# TESTING THE MODEL\n",
    "# ------------------------------\n",
    "final_stats = []\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6)) \n",
    "ax_roc, ax_pr = axes\n",
    "\n",
    "for mod_name, trained_mod in trained_models.items():\n",
    "    feature_cols, target_col = get_cols(pred_year, outcomes, qs, encoded, prev)\n",
    "    X, y = df_2022_2023[feature_cols], df_2022_2023[target_col]\n",
    "    y_pred = trained_mod.predict(X)\n",
    "\n",
    "    # Classification\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    report = classification_report(y, y_pred, target_names=[\"no\", \"yes\"], output_dict=True)\n",
    "\n",
    "    # PR Curve\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred)\n",
    "    pr_score = auc(recall, precision)\n",
    "    ax_pr.plot(recall, precision, marker='.', label=f'{mod_name}: {pr_score:.2f}')\n",
    "\n",
    "    # ROC Curve\n",
    "    y_pred_proba = trained_mod.predict_proba(X)[:, 1]\n",
    "    roc_score = roc_auc_score(y, y_pred_proba)\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "    ax_roc.plot(fpr, tpr, label=f\"{mod_name} (ROC = {roc_score:.3f})\")\n",
    "\n",
    "    final_stats.append({\n",
    "        \"Model\": mod_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC AUC\": roc_score,\n",
    "        \"Precision\": report[\"yes\"][\"precision\"],\n",
    "        \"Recall\": report[\"yes\"][\"recall\"],\n",
    "        \"F1\": report[\"yes\"][\"f1-score\"],\n",
    "        \"PR AUC\": pr_score,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(final_stats))\n",
    "\n",
    "# Finalizing ROC subplot\n",
    "ax_roc.plot([0, 1], [0, 1], linestyle='--', color='gray')  \n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "ax_roc.set_title(\"ROC Curves for Models\")\n",
    "ax_roc.legend(loc=\"lower right\")\n",
    "ax_roc.grid(True)\n",
    "\n",
    "# Finalizing PR subplot\n",
    "ax_pr.plot([0, 0], [1, 1], linestyle='--', color='gray')  \n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "ax_pr.set_ylim(0, 1)\n",
    "ax_pr.set_title(\"Precision-Recall Curves for Models\")\n",
    "ax_pr.legend()\n",
    "ax_pr.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007BFF; height: 4px; width: 100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Compounding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing suicidality responses (classification and probability)\n",
    "\n",
    "# Predicting current year responses using current year responses to non-suicidality questions\n",
    "curr_imputation = {\n",
    "    \"Attempt\": ...,\n",
    "    \"How\": ...,\n",
    "    \"Thoughts\": ...,\n",
    "    \"Selfharm\": ...,\n",
    "}\n",
    "\n",
    "# Predicting current year responses using current year responses to non-suicidality questions and previous year responses (including suicidality questions)\n",
    "prev_imputation = {\n",
    "    \"Attempt\": ...,\n",
    "    \"How\": ...,\n",
    "    \"Thoughts\": ...,\n",
    "    \"Selfharm\": ...,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of notebook :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #007BFF; height: 4px; width: 100%;\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs109a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
